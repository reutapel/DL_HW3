2017-01-27 00:31:15 [program started on Fri Jan 27 00:31:15 2017] 
2017-01-27 00:31:15 [command line arguments] 
2017-01-27 00:31:15 seed 123 
2017-01-27 00:31:15 earlyStop 5 
2017-01-27 00:31:15 initWeight 0.08 
2017-01-27 00:31:15 batchSize 50 
2017-01-27 00:31:15 LRDecay 0 
2017-01-27 00:31:15 numLayers 2 
2017-01-27 00:31:15 gradClip 5 
2017-01-27 00:31:15 decayRate 2 
2017-01-27 00:31:15 model LSTM 
2017-01-27 00:31:15 constBatchSize false 
2017-01-27 00:31:15 LR 0.002 
2017-01-27 00:31:15 epochDecay 5 
2017-01-27 00:31:15 seqLength 50 
2017-01-27 00:31:15 load  
2017-01-27 00:31:15 bestEpoch 1 
2017-01-27 00:31:15 devid 1 
2017-01-27 00:31:15 save /home/reutapel@st.technion.ac.il/DL_HW3/Results/FriJan2700:31:132017 
2017-01-27 00:31:15 epoch 25 
2017-01-27 00:31:15 checkpoint 0 
2017-01-27 00:31:15 type cuda 
2017-01-27 00:31:15 momentum 0 
2017-01-27 00:31:15 rnnSize 220 
2017-01-27 00:31:15 weightDecay 0 
2017-01-27 00:31:15 threads 8 
2017-01-27 00:31:15 optimization rmsprop 
2017-01-27 00:31:15 dropout 0.2 
2017-01-27 00:31:15 shuffle false 
2017-01-27 00:31:15 optState false 
2017-01-27 00:31:15 nGPU 1 
2017-01-27 00:31:15 [----------------------] 
2017-01-27 00:31:15 
==> Network 
2017-01-27 00:31:15 nn.Sequential {
  [input -> (1) -> (2) -> (3) -> output]
  (1): nn.LookupTable
  (2): nn.Sequential {
    [input -> (1) -> (2) -> (3) -> (4) -> output]
    (1): nn.LSTM(220 -> 220, 440)
    (2): nn.Dropout(0.200000)
    (3): nn.LSTM(220 -> 220, 440)
    (4): nn.Dropout(0.200000)
  }
  (3): nn.TemporalModule {
    [input -> (1) -> output]
    (1): nn.Linear(220 -> 10000)
  }
} 
2017-01-27 00:31:15 
==>2986160 Parameters 
2017-01-27 00:31:15 
==> Criterion 
2017-01-27 00:31:15 nn.CrossEntropyCriterion 
2017-01-27 00:31:15 
Epoch 1
 
2017-01-27 00:32:08 
Training Perplexity: 393.37033081055 
2017-01-27 00:32:10 
Validation Perplexity: 244.19584655762 
2017-01-27 00:32:11 
Test Perplexity: 230.18531799316 
2017-01-27 00:32:14 
Epoch 2
 
2017-01-27 00:33:04 
Training Perplexity: 201.33558654785 
2017-01-27 00:33:06 
Validation Perplexity: 180.42718505859 
2017-01-27 00:33:07 
Test Perplexity: 170.37922668457 
2017-01-27 00:33:07 
Epoch 3
 
2017-01-27 00:33:58 
Training Perplexity: 154.71702575684 
2017-01-27 00:33:59 
Validation Perplexity: 155.56413269043 
2017-01-27 00:34:01 
Test Perplexity: 147.33819580078 
2017-01-27 00:34:01 
Epoch 4
 
2017-01-27 00:34:51 
Training Perplexity: 129.2375793457 
2017-01-27 00:34:53 
Validation Perplexity: 139.66239929199 
2017-01-27 00:34:54 
Test Perplexity: 132.30386352539 
2017-01-27 00:34:54 
Epoch 5
 
2017-01-27 00:35:45 
Training Perplexity: 112.98890686035 
2017-01-27 00:35:46 
Validation Perplexity: 129.52253723145 
2017-01-27 00:35:47 
Test Perplexity: 122.75788879395 
2017-01-27 00:35:48 Learning Rate decreased to: 0.001 
2017-01-27 00:35:48 
Epoch 6
 
2017-01-27 00:36:38 
Training Perplexity: 98.993919372559 
2017-01-27 00:36:39 
Validation Perplexity: 122.79026794434 
2017-01-27 00:36:41 
Test Perplexity: 116.33728027344 
2017-01-27 00:36:41 Learning Rate decreased to: 0.0005 
2017-01-27 00:36:41 
Epoch 7
 
2017-01-27 00:37:32 
Training Perplexity: 92.204704284668 
2017-01-27 00:37:33 
Validation Perplexity: 119.84304046631 
2017-01-27 00:37:34 
Test Perplexity: 113.34143829346 
2017-01-27 00:37:35 Learning Rate decreased to: 0.00025 
2017-01-27 00:37:35 
Epoch 8
 
2017-01-27 00:38:25 
Training Perplexity: 89.21768951416 
2017-01-27 00:38:26 
Validation Perplexity: 118.38205718994 
2017-01-27 00:38:28 
Test Perplexity: 112.04907226562 
2017-01-27 00:38:28 Learning Rate decreased to: 0.000125 
2017-01-27 00:38:28 
Epoch 9
 
2017-01-27 00:39:19 
Training Perplexity: 87.7626953125 
2017-01-27 00:39:20 
Validation Perplexity: 117.67669677734 
2017-01-27 00:39:21 
Test Perplexity: 111.4412612915 
2017-01-27 00:39:22 Learning Rate decreased to: 6.25e-05 
2017-01-27 00:39:22 
Epoch 10
 
2017-01-27 00:40:12 
Training Perplexity: 87.089775085449 
2017-01-27 00:40:14 
Validation Perplexity: 117.13478088379 
2017-01-27 00:40:15 
Test Perplexity: 111.01377868652 
2017-01-27 00:40:15 Learning Rate decreased to: 3.125e-05 
2017-01-27 00:40:15 
Epoch 11
 
2017-01-27 00:41:06 
Training Perplexity: 86.676475524902 
2017-01-27 00:41:07 
Validation Perplexity: 116.89011383057 
2017-01-27 00:41:08 
Test Perplexity: 110.79949188232 
2017-01-27 00:41:09 Learning Rate decreased to: 1.5625e-05 
2017-01-27 00:41:09 
Epoch 12
 
2017-01-27 00:41:59 
Training Perplexity: 86.481452941895 
2017-01-27 00:42:00 
Validation Perplexity: 116.7815322876 
2017-01-27 00:42:02 
Test Perplexity: 110.6781463623 
2017-01-27 00:42:02 Learning Rate decreased to: 7.8125e-06 
2017-01-27 00:42:02 
Epoch 13
 
2017-01-27 00:42:53 
Training Perplexity: 86.29126739502 
2017-01-27 00:42:54 
Validation Perplexity: 116.71723175049 
2017-01-27 00:42:55 
Test Perplexity: 110.61504364014 
2017-01-27 00:42:55 Learning Rate decreased to: 3.90625e-06 
2017-01-27 00:42:55 
Epoch 14
 
2017-01-27 00:43:46 
Training Perplexity: 86.312622070312 
2017-01-27 00:43:47 
Validation Perplexity: 116.69346618652 
2017-01-27 00:43:49 
Test Perplexity: 110.59036254883 
2017-01-27 00:43:49 Learning Rate decreased to: 1.953125e-06 
2017-01-27 00:43:49 
Epoch 15
 
2017-01-27 00:44:40 
Training Perplexity: 86.175758361816 
2017-01-27 00:44:41 
Validation Perplexity: 116.67833709717 
2017-01-27 00:44:42 
Test Perplexity: 110.57712554932 
2017-01-27 00:44:42 Learning Rate decreased to: 9.765625e-07 
2017-01-27 00:44:42 
Epoch 16
 
2017-01-27 00:45:34 
Training Perplexity: 86.243881225586 
2017-01-27 00:45:35 
Validation Perplexity: 116.671043396 
2017-01-27 00:45:36 
Test Perplexity: 110.5705871582 
2017-01-27 00:45:37 Learning Rate decreased to: 4.8828125e-07 
2017-01-27 00:45:37 
Epoch 17
 
2017-01-27 00:46:27 
Training Perplexity: 86.135459899902 
2017-01-27 00:46:29 
Validation Perplexity: 116.66742706299 
2017-01-27 00:46:30 
Test Perplexity: 110.56790161133 
2017-01-27 00:46:30 Learning Rate decreased to: 2.44140625e-07 
2017-01-27 00:46:30 
Epoch 18
 
2017-01-27 00:47:21 
Training Perplexity: 86.229606628418 
2017-01-27 00:47:22 
Validation Perplexity: 116.6658706665 
2017-01-27 00:47:24 
Test Perplexity: 110.56679534912 
2017-01-27 00:47:24 Learning Rate decreased to: 1.220703125e-07 
2017-01-27 00:47:24 
Epoch 19
 
2017-01-27 00:48:14 
Training Perplexity: 86.220069885254 
2017-01-27 00:48:16 
Validation Perplexity: 116.66520690918 
2017-01-27 00:48:17 
Test Perplexity: 110.56621551514 
2017-01-27 00:48:17 Learning Rate decreased to: 6.103515625e-08 
2017-01-27 00:48:17 
Epoch 20
 
2017-01-27 00:49:28 
Training Perplexity: 86.30192565918 
2017-01-27 00:49:30 
Validation Perplexity: 116.66481781006 
2017-01-27 00:49:32 
Test Perplexity: 110.56589508057 
2017-01-27 00:49:32 Learning Rate decreased to: 3.0517578125e-08 
2017-01-27 00:49:32 
Epoch 21
 
2017-01-27 00:51:02 
Training Perplexity: 86.177322387695 
2017-01-27 00:51:04 
Validation Perplexity: 116.66464996338 
2017-01-27 00:51:06 
Test Perplexity: 110.56574249268 
2017-01-27 00:51:06 Learning Rate decreased to: 1.52587890625e-08 
2017-01-27 00:51:06 
Epoch 22
 
2017-01-27 00:52:37 
Training Perplexity: 86.163269042969 
2017-01-27 00:52:39 
Validation Perplexity: 116.66453552246 
2017-01-27 00:52:41 
Test Perplexity: 110.56563568115 
2017-01-27 00:52:41 Learning Rate decreased to: 7.62939453125e-09 
2017-01-27 00:52:41 
Epoch 23
 
2017-01-27 00:54:12 
Training Perplexity: 86.326126098633 
2017-01-27 00:54:14 
Validation Perplexity: 116.6644821167 
2017-01-27 00:54:16 
Test Perplexity: 110.56563568115 
2017-01-27 00:54:16 Learning Rate decreased to: 3.814697265625e-09 
2017-01-27 00:54:16 
Epoch 24
 
2017-01-27 00:55:47 
Training Perplexity: 86.275917053223 
2017-01-27 00:55:48 
Validation Perplexity: 116.6644821167 
2017-01-27 00:55:51 
Test Perplexity: 110.56563568115 
2017-01-27 00:55:51 Learning Rate decreased to: 1.9073486328125e-09 
2017-01-27 00:55:51 
Epoch 25
 
2017-01-27 00:57:21 
Training Perplexity: 86.192321777344 
2017-01-27 00:57:23 
Validation Perplexity: 116.6644821167 
2017-01-27 00:57:25 start sampling... 
2017-01-27 00:57:25 
Sampled Text:
 
2017-01-27 00:57:25 Buy low, sell high is the... big financial average on stock exchange at the end of wednesday 
 
2017-01-27 00:57:25 
Sampled Text:
 
2017-01-27 00:57:25 Buy low, sell high is the... for texas equivalents 
 in a restructuring in nasdaq companies more than telephone 
2017-01-27 00:57:25 
Sampled Text:
 
2017-01-27 00:57:25 Buy low, sell high is the... gas hit at least most day 
 these things left to buy affected the 
2017-01-27 00:57:25 
Sampled Text:
 
2017-01-27 00:57:25 Buy low, sell high is the... trade world 's bargain role 
 the term traders said the growing focus 's highest 
2017-01-27 00:57:25 
Sampled Text:
 
2017-01-27 00:57:25 Buy low, sell high is the... issue for we see he adds 
 one the scope of the sci world 's miniscribe 
2017-01-27 00:57:25 
Test Perplexity: 110.56563568115 
2017-01-27 00:57:25 Learning Rate decreased to: 9.5367431640625e-10 
2017-01-27 00:57:25 Best Iteration was 24, With a validation loss of: 4.7056092619896 
